{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db227211",
   "metadata": {},
   "source": [
    "Reese Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9439442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#External libraries\n",
    "# This cell imports libraries that this code uses\n",
    "\n",
    "import numpy as np                   # functions for data analysis \n",
    "import pandas as pd                  # functions for data frames\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Constants\n",
    "\n",
    "startdate = datetime.datetime(1969, 1, 1)\n",
    "enddate = datetime.datetime(2020, 12, 31)\n",
    "\n",
    "headerlist = ['Date Time', 'Water Level', 'I', 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Ingesting, organizing, and filling missing water level data\n",
    "\n",
    "waterlevelfiles =  ['olc68to77.csv', 'olc78to87.csv', 'olc88to97.csv', 'olc98to07.csv',\n",
    "               'olc08to17.csv', 'olc18to20.csv', 'Cape68to77.csv', 'Cape78to87.csv',\n",
    "               'Cape88to97.csv', 'Cape98to07.csv', 'Cape08to17.csv', 'Cape18to20.csv',\n",
    "               'Oz68to77.csv', 'Oz78to87.csv', 'Oz88to97.csv', 'Oz98to07.csv', \n",
    "               'Oz08to17.csv', 'Oz18to20.csv', 'Roch68to77.csv', 'Roch78to87.csv', \n",
    "               'Roch88to97.csv', 'Roch98to07.csv', 'Roch08to17.csv', 'Roch18to20.csv', \n",
    "               ]\n",
    "\n",
    "dscgfiles = ['niagdscg.csv', 'lawdscg.csv', 'genndscg.csv', 'blkdscg.csv', 'ozdscg.csv']\n",
    "\n",
    "prcpfile = ['ozprcp.csv', 'rochprcp.csv', 'wtprcp.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad6b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% Importing Water Level Data\n",
    "\n",
    "olcdflvl0 = pd.read_csv(waterlevelfiles[0], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl1 = pd.read_csv(waterlevelfiles[1], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl2 = pd.read_csv(waterlevelfiles[2], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl3 = pd.read_csv(waterlevelfiles[3], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl4 = pd.read_csv(waterlevelfiles[4], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl5 = pd.read_csv(waterlevelfiles[5], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the date, renaming columns\n",
    "olcdflvl = pd.concat([olcdflvl0, olcdflvl1, olcdflvl2, olcdflvl3, olcdflvl4, \n",
    "                      olcdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "olcdflvl = olcdflvl[startdate:enddate]\n",
    "olcdflvl.rename(columns = {'Water Level' : 'Olcott'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "capedflvl0 = pd.read_csv(waterlevelfiles[6], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl1 = pd.read_csv(waterlevelfiles[7], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl2 = pd.read_csv(waterlevelfiles[8], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl3 = pd.read_csv(waterlevelfiles[9], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl4 = pd.read_csv(waterlevelfiles[10], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl5 = pd.read_csv(waterlevelfiles[11], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "capedflvl = pd.concat([capedflvl0, capedflvl1, capedflvl2, capedflvl3, capedflvl4,\n",
    "                       capedflvl5], axis=0, join='outer', ignore_index=False)\n",
    "capedflvl = capedflvl[startdate:enddate]\n",
    "capedflvl.rename(columns = {'Water Level' : 'Cape Vincent'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "ozdflvl0 = pd.read_csv(waterlevelfiles[12], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl1 = pd.read_csv(waterlevelfiles[13], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl2 = pd.read_csv(waterlevelfiles[14], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl3 = pd.read_csv(waterlevelfiles[15], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl4 = pd.read_csv(waterlevelfiles[16], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl5 = pd.read_csv(waterlevelfiles[17], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "ozdflvl = pd.concat([ozdflvl0, ozdflvl1, ozdflvl2, ozdflvl3, ozdflvl4,\n",
    "                       ozdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "ozdflvl = ozdflvl[startdate:enddate]\n",
    "ozdflvl.rename(columns = {'Water Level' : 'Oswego'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "rochdflvl0 = pd.read_csv(waterlevelfiles[18], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl1 = pd.read_csv(waterlevelfiles[19], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl2 = pd.read_csv(waterlevelfiles[20], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl3 = pd.read_csv(waterlevelfiles[21], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl4 = pd.read_csv(waterlevelfiles[22], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl5 = pd.read_csv(waterlevelfiles[23], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "rochdflvl = pd.concat([rochdflvl0, rochdflvl1, rochdflvl2, rochdflvl3, rochdflvl4,\n",
    "                       rochdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "rochdflvl = rochdflvl[startdate:enddate]\n",
    "rochdflvl.rename(columns = {'Water Level' : 'Rochester'}, inplace = True)\n",
    "\n",
    "#Keeping only water level columns which has been named with site names\n",
    "olcdflvl = olcdflvl[['Olcott']]\n",
    "capedflvl = capedflvl[['Cape Vincent']]\n",
    "ozdflvl = ozdflvl[['Oswego']]\n",
    "rochdflvl = rochdflvl[['Rochester']]\n",
    "\n",
    "#resamlping to ensure all days are covered, and linearly interpolating any missing data\n",
    "olcdflvl = olcdflvl.resample('1D').interpolate('linear') #large stretch from mid '99 to mid '00 missing\n",
    "capedflvl = capedflvl.resample('1D').interpolate('linear')\n",
    "ozdflvl = ozdflvl.resample('1D').interpolate('linear')\n",
    "rochdflvl = rochdflvl.resample('1D').interpolate('linear')\n",
    "\n",
    "#Joining into one main dataframe with all waterlevel data\n",
    "waterlevel = pd.concat([olcdflvl, capedflvl, ozdflvl, rochdflvl], axis = 1) \n",
    "\n",
    "#Calculating standard dev\n",
    "stdev = waterlevel.std(axis = 1)\n",
    "\n",
    "#Getting average lake level\n",
    "avgLL = pd.DataFrame()\n",
    "avgLL['avg'] = waterlevel.mean(axis = 1)\n",
    "\n",
    "maximumwldiff = max(stdev) #ft\n",
    "avgwldiff = stdev.mean()\n",
    "maximumwldiffdate = datetime.datetime(2000, 2, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9332fc",
   "metadata": {},
   "source": [
    "Standard deviations were calculated for each day across all sites to determine appropriateness of averaging the data into one site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The maximum difference in water level between sites is:', maximumwldiff, '(ft).')\n",
    "print('The date of maximum water level difference was', maximumwldiffdate, '.')\n",
    "print('The average difference in water level between sites is:', avgwldiff, '(ft).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea2d3c",
   "metadata": {},
   "source": [
    "These results showed that the water level heights were all similar enough to average together into one column. A maximum standard deviation of less than 1 foott, and an average standard deviation of less than 1/10th of a foot showed homogeneity across the lake shoreline. This also helped buffer the missing period of (xxx to xxx) at the Olcott site. After finding the date of maximum standard deviation, observation showed that a confounding factor likely influenced this measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing Discharge data\n",
    "\n",
    "niagdscg = pd.read_csv(dscgfiles[0], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "niagdscg = niagdscg[['14n']]\n",
    "niagdscg.rename(columns={'14n':'Q Niag'}, inplace = True)\n",
    "\n",
    "\n",
    "genndscg = pd.read_csv(dscgfiles[2], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "genndscg = genndscg[['14n']]\n",
    "genndscg.rename(columns={'14n':'Q Genn'}, inplace = True)\n",
    "\n",
    "\n",
    "blackdscg = pd.read_csv(dscgfiles[3], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "blackdscg = blackdscg[['14n']]\n",
    "blackdscg.rename(columns={'14n':'Q Black'}, inplace = True)\n",
    "\n",
    "\n",
    "ozdscg = pd.read_csv(dscgfiles[4], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdscg = ozdscg[['14n']]\n",
    "ozdscg.rename(columns={'14n':'Q Oz'}, inplace = True)\n",
    "\n",
    "\n",
    "lawrencedscg = pd.read_csv(dscgfiles[1], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "lawrencedscg = lawrencedscg[['14n']]\n",
    "lawrencedscg['14n'] = lawrencedscg['14n']*86400\n",
    "lawrencedscg.rename(columns={'14n':'Q Out'}, inplace = True)\n",
    "\n",
    "\n",
    "discharge = pd.concat([niagdscg, lawrencedscg, genndscg, blackdscg, ozdscg], axis = 1)\n",
    "\n",
    "discharge['qsum'] = discharge.iloc[:, [0, 2, 3, 4]].sum(axis = 1)\n",
    "discharge['Q In'] = discharge['qsum']*86400 #cubic feet per day\n",
    "qdiff = discharge['Q In'] - discharge['Q Out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c86c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing precip data\n",
    "\n",
    "ozprcp = pd.read_csv(prcpfile[0], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozprcp = ozprcp[['PRCP']]\n",
    "ozprcp.rename(columns={'PRCP':'Oz P'}, inplace = True)\n",
    "ozprcp = ozprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "rochprcp = pd.read_csv(prcpfile[1], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochprcp = rochprcp[['PRCP']]\n",
    "rochprcp.rename(columns={'PRCP':'Roch P'}, inplace = True)\n",
    "rochprcp = rochprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "wtprcp = pd.read_csv(prcpfile[2], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "wtprcp = wtprcp[['PRCP']]\n",
    "wtprcp.rename(columns={'PRCP':'WT P'}, inplace = True)\n",
    "wtprcp = wtprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "precipitation = pd.concat([ozprcp, rochprcp, wtprcp], axis = 1)\n",
    "\n",
    "prcptotal = precipitation.sum(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1370931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Making one complete dataframe with average water level, combined discharge\n",
    "#and total precipitation\n",
    "\n",
    "dfall = pd.concat([avgLL, prcptotal, qdiff], axis = 1)\n",
    "dfall.rename(columns={0:'total'}, inplace = True)\n",
    "dfall.rename(columns={1:'q'}, inplace = True)\n",
    "dfall['Q Out'] = discharge['Q Out']\n",
    "dfall['Q In'] = discharge['Q In']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba32194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "fig, (ax) = plt.subplots()\n",
    "\n",
    "ax.plot(avgLL, color = 'r')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(qdiff, linestyle = 'dotted')\n",
    "ax3 = ax.twinx()\n",
    "ax3.plot(prcptotal, color = 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148836c7",
   "metadata": {},
   "source": [
    "This graph shows a closeup view of the average water height of the lake, and the change in discharge from the lake. The change in discharge from the lake was calculated by converting all average daily discharge per second rates into volume per day rates by multiplying by 86400, then subtracting the outflows from the inflows. As the line dips below 0, that represents greater discharge OUT of the lake, while positive it represents greater discharge INTO the lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Defining flood events and years\n",
    "\n",
    "#plan 2014 was agreed to and enacted in 2016, previous plan was 1958D which was\n",
    "# in use since 1963 (floods 1973-1998 all plan 1958D)\n",
    "floodyear = pd.DataFrame()\n",
    "floodyear['start'] = ['1973-01-01', '1976-01-01','1983-01-01', \n",
    "                      '1998-01-01','2017-01-01', '2019-01-01',]\n",
    "floodyear['end'] = ['1973-12-31', '1976-12-31', '1983-12-31', \n",
    "                    '1998-12-31','2017-12-31', '2019-12-31']\n",
    "\n",
    "floodyear['start'] = pd.to_datetime(floodyear['start'])\n",
    "floodyear['end'] = pd.to_datetime(floodyear['end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbab95",
   "metadata": {},
   "source": [
    "Local news and agency report sear lead to determining most years of major coastline flooding on the lake, which were then plotted to determine trends. Plan 1958d had been in use by the IJC since 1963, while Plan 2014 was enacted in 2016. Plan 1958d covered 4 major flood years, while Plan 2014 covered 2 major storm years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Data Comparison for Lake Ontario for '\n",
    "\n",
    "def timeplot(df, startdates, enddates):\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, sharex = True, figsize = (11,8))\n",
    "    \n",
    "    #plotting change in discharge (negative = more out, positive = more in)\n",
    "    ax1.plot(df['avg'], label = 'Lake Height')\n",
    "    ax1.set_ylabel('Lake Height (ft)')\n",
    "    ax1.twinx().plot(df['q'], color = 'r', label = 'Delta Discharge')\n",
    "    #ax1.set_ylabel('Change in Discharge (cf/day)')\n",
    "    \n",
    "    #plotting precip \n",
    "    ax2.plot(df['avg'])\n",
    "    ax2.twinx().plot(df['total'], color = 'k', label = 'Precip')\n",
    "    ax2.set_ylabel('Lake Height (ft)')\n",
    "    #ax2.set_ylabel('Precipitation (in)')\n",
    "    \n",
    "    #plotting all NEED TO FIGURE OUT THIRD AXIS (CODE BELOW) \n",
    "    ax3.twinx().plot(df['total'], color = 'k')\n",
    "    ax3.twinx().plot(df['q'], color = 'r')\n",
    "    ax3.plot(df['avg'])\n",
    "    ax3.set_ylabel('Lake Height (ft)')\n",
    "    #ax3.set_ylabel('Change in Discharge (cf/day)')\n",
    "    \n",
    "    #Lake level height vs actual discharge out of lake\n",
    "    ax4.plot(df['avg'])\n",
    "    ax4.twinx().plot(df['Q Out'], color = 'r', linestyle = 'dotted', label = \n",
    "                     'Stl Discharge')\n",
    "    ax4.set_ylabel('Lake Height (ft)')\n",
    "    #ax4.set_ylabel('Discharge Out (cf/day)')\n",
    "    \n",
    "    #Sum of river discharges plotted against discharge leaving lake\n",
    "    ax5.plot(df['Q In'])\n",
    "    ax5.twinx().plot(df['Q Out'], color = 'r', linestyle = 'dotted')\n",
    "    ax5.set_ylabel('Discharge In')\n",
    "    \n",
    "    fig.legend(bbox_to_anchor=(1.085,0.5))\n",
    "    fig.suptitle(title + str(startdates.date()) + ' - ' + str(enddates.date()), size = 24)\n",
    "    \n",
    "timeplot(dfall, startdate, enddate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77190dcf",
   "metadata": {},
   "source": [
    "This figure shows Lake Ontario data between 1969 and 2020. The first graph is a comparison of the average lake water level height, and the change in discharge. The second graph shows the average lake water level height and the precipitation from 3 major cities along the coastline. The third graph compares average lake water level height, precipitation, and change in discharge. The fourth graph shows the average lake water height and the actual daily volumetric discharge from the Saunders Moses Dam which controls all flow out of Lake Ontario. The fifth graph compares the sum of all major river inflows to the lake versus the outflow from the Saunders Moses Dam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ed683",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(floodyear['start']):\n",
    "    finaldate = floodyear.iloc[i,1]\n",
    "    loop = dfall[v:finaldate]\n",
    "    timeplot(loop, v, finaldate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fc570",
   "metadata": {},
   "source": [
    "The graphs above show similar data to the previous figure, but narrowed in on high flood years."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
