{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db227211",
   "metadata": {},
   "source": [
    "Reese Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b71caf",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "(Have a photo of Lake Ontario and St. Lawrence River here)\n",
    "\n",
    "Lake Ontario acts as the outlet for all water from the Great Lakes. The St Lawrence River is the sole outlet for the lake, and has its flow regulated by the Moses Saunders Dam. This effectively controls the water level height of the lake. Throughout history, water height levels in the lake have fluctuated with some years reaching coastal flood levels. Those years were: 1973, 1976, 1983, 1998, 2017, 2019. The regulation of the outflow from the lake is managed through an internation organization between Canada and the United States known as the Internation Joint Commission  (IJC). Representation on the board of the commission is shared equally between both countries.\n",
    "\n",
    "There have been two major regulation plans for discharge out of the St. Lawrence river, Plan 1958D and Plan 2014. Plan 1958D had been in effect from 1960 to 2016, and Plan 2014 has been in effect since 2016. After the transition of control plans, Lake Ontario experienced two of its worst flood years in history in 2017 and 2019. This led to anecdotal evidence of the change in control plans being the cause of the flooding instead of unusually wet conditions throughout the year. \n",
    "\n",
    "This study seeks to understand and quantify the changes the control plans had during years of excessive lake height. By analyzing a variety of data from the US side of Lake Ontario, as well as the discharge logs from the Moses Saunders Dam maintained by the IJC, a determination will be made on the various factors influencing flood levels in the lake by creating a rudimentary water budget for Lake Ontario. The overall goal of this study is to prove or disprove the anectdotal evidence that Plan 2014 has caused the intense flooding along the lake's coastline due to the change in operating procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae2dcc",
   "metadata": {},
   "source": [
    "# Site Description\n",
    "\n",
    "The main site this study will seek to address is Lake Ontario and the shorefront communities, this \n",
    "will be done by incorporating data from 7 general locations along the American side of the lake. \n",
    "These  sites  are  all  located  in  New  York,  and  are  based  out  of  Niagara  Falls,  Olcott,  Rochester, \n",
    "Oswego, Watertown, Cape Vincent, and Massena. These sites were selected due to the \n",
    "availability  of  data  from  various  governmental  agencies,  as  well  as  providing  a  representative \n",
    "sample  distributed  across  the  southern  and  western  coasts  of  Lake  Ontario.  The  communities \n",
    "affected by the flooding mentioned in this study lie between gaging locations. The data available \n",
    "for these sites are daily values for precipitation, discharge, and water level height. The \n",
    "approximate location of the gages is shown in Figure 1, with blue symbols for precipitation gages, \n",
    "red  symbols  for  discharge  gages,  green  symbols  for  water  height  gages.  All  data  collected  was \n",
    "available  from  1968  to  2021,  which  encompasses  6  of  the  7  major  peak  floods  on  record.  The \n",
    "drainage basin for the St. Lawrence River gage station is 298,800 mi2, encompassing all the Great \n",
    "Lakes as well as lesser lakes and tributaries draining directly into the St. Lawrence. The basin of \n",
    "Niagara River site represents 263,700 mi2 of the total for the St. Lawrence River basin, while the \n",
    "Oswego River (5,100 mi2), Genesee River (2,474 mi2), and the Black River (1,864 mi2) contribute \n",
    "approximately 9,500 mi2 to the total basin area.\n",
    "\n",
    "Table 1. A table showing the data type, location, and gage number for all data sets used for the study. Water \n",
    "height data were collected from the NOAA Tides and Currents database, discharge data was collected from the \n",
    "USGS gaging station database, and precipitation data was gathered from the NOAA Climate Data Online database.\n",
    "Data Type Location Gage Number\n",
    "Water Height Olcott, NY 9052076\n",
    "Water Height Rochester, NY 9052058\n",
    "Water Height Oswego, NY 9052030\n",
    "Water Height Cape Vincent, NY 9052000\n",
    "Discharge Niagara River, NY 04216000\n",
    "Discharge Genessee River, NY 04231600\n",
    "Discharge Oswego River, NY 04249000\n",
    "Discharge Black River, NY 04260500\n",
    "Discharge St. Lawrence River, NY 04264331\n",
    "Precipitation Rochester, NY USW00014768\n",
    "Precipitation Oswego, NY USC00306314\n",
    "Precipitation Watertown, NY USW00094790"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce1b1e",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9439442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#External libraries\n",
    "# This cell imports libraries that this code uses\n",
    "\n",
    "import numpy as np                   # functions for data analysis \n",
    "import pandas as pd                  # functions for data frames\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Constants\n",
    "\n",
    "startdate = datetime.datetime(1969, 1, 1)\n",
    "enddate = datetime.datetime(2020, 12, 31)\n",
    "\n",
    "headerlist = ['Date Time', 'Water Level', 'I', 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Ingesting, organizing, and filling missing water level data\n",
    "\n",
    "waterlevelfiles =  ['olc68to77.csv', 'olc78to87.csv', 'olc88to97.csv', 'olc98to07.csv',\n",
    "               'olc08to17.csv', 'olc18to20.csv', 'Cape68to77.csv', 'Cape78to87.csv',\n",
    "               'Cape88to97.csv', 'Cape98to07.csv', 'Cape08to17.csv', 'Cape18to20.csv',\n",
    "               'Oz68to77.csv', 'Oz78to87.csv', 'Oz88to97.csv', 'Oz98to07.csv', \n",
    "               'Oz08to17.csv', 'Oz18to20.csv', 'Roch68to77.csv', 'Roch78to87.csv', \n",
    "               'Roch88to97.csv', 'Roch98to07.csv', 'Roch08to17.csv', 'Roch18to20.csv', \n",
    "               ]\n",
    "\n",
    "dscgfiles = ['niagdscg.csv', 'lawdscg.csv', 'genndscg.csv', 'blkdscg.csv', 'ozdscg.csv']\n",
    "\n",
    "prcpfile = ['ozprcp.csv', 'rochprcp.csv', 'wtprcp.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd7b5e",
   "metadata": {},
   "source": [
    "For this study, data was gathered from a variety of databases and brought together to create a water budget for Lake Ontario. Water level height data in the lake was gathered from the Tides and Currents database within the National Oceanographic and Atmospheric Administration for four sites along the New York side of the lake: Olcott, Rochester, Oswego, and Cape Vincent. These sites represent the southern and eastern shores of Lake Ontario. Data were then analyzed for variation among sites to determine if it was suitable to use the average of all sites as an analogue for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad6b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% Importing Water Level Data\n",
    "\n",
    "olcdflvl0 = pd.read_csv(waterlevelfiles[0], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl1 = pd.read_csv(waterlevelfiles[1], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl2 = pd.read_csv(waterlevelfiles[2], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl3 = pd.read_csv(waterlevelfiles[3], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl4 = pd.read_csv(waterlevelfiles[4], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "olcdflvl5 = pd.read_csv(waterlevelfiles[5], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "olcdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the date, renaming columns\n",
    "olcdflvl = pd.concat([olcdflvl0, olcdflvl1, olcdflvl2, olcdflvl3, olcdflvl4, \n",
    "                      olcdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "olcdflvl = olcdflvl[startdate:enddate]\n",
    "olcdflvl.rename(columns = {'Water Level' : 'Olcott'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "capedflvl0 = pd.read_csv(waterlevelfiles[6], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl1 = pd.read_csv(waterlevelfiles[7], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl2 = pd.read_csv(waterlevelfiles[8], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl3 = pd.read_csv(waterlevelfiles[9], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl4 = pd.read_csv(waterlevelfiles[10], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "capedflvl5 = pd.read_csv(waterlevelfiles[11], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "capedflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "capedflvl = pd.concat([capedflvl0, capedflvl1, capedflvl2, capedflvl3, capedflvl4,\n",
    "                       capedflvl5], axis=0, join='outer', ignore_index=False)\n",
    "capedflvl = capedflvl[startdate:enddate]\n",
    "capedflvl.rename(columns = {'Water Level' : 'Cape Vincent'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "ozdflvl0 = pd.read_csv(waterlevelfiles[12], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl1 = pd.read_csv(waterlevelfiles[13], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl2 = pd.read_csv(waterlevelfiles[14], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl3 = pd.read_csv(waterlevelfiles[15], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl4 = pd.read_csv(waterlevelfiles[16], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "ozdflvl5 = pd.read_csv(waterlevelfiles[17], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "ozdflvl = pd.concat([ozdflvl0, ozdflvl1, ozdflvl2, ozdflvl3, ozdflvl4,\n",
    "                       ozdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "ozdflvl = ozdflvl[startdate:enddate]\n",
    "ozdflvl.rename(columns = {'Water Level' : 'Oswego'}, inplace = True)\n",
    "\n",
    "##############################################################################  Breaks between sites\n",
    "\n",
    "rochdflvl0 = pd.read_csv(waterlevelfiles[18], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl0.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl1 = pd.read_csv(waterlevelfiles[19], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl1.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl2 = pd.read_csv(waterlevelfiles[20], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl2.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl3 = pd.read_csv(waterlevelfiles[21], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl3.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl4 = pd.read_csv(waterlevelfiles[22], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl4.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "rochdflvl5 = pd.read_csv(waterlevelfiles[23], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['Date Time'], index_col= 'Date Time', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochdflvl5.rename(columns=lambda x: x.strip(), inplace=True) #stripping spaces out of names\n",
    "\n",
    "#Joining into one site dataframe, trimming to the dates, renaming columns\n",
    "rochdflvl = pd.concat([rochdflvl0, rochdflvl1, rochdflvl2, rochdflvl3, rochdflvl4,\n",
    "                       rochdflvl5], axis=0, join='outer', ignore_index=False)\n",
    "rochdflvl = rochdflvl[startdate:enddate]\n",
    "rochdflvl.rename(columns = {'Water Level' : 'Rochester'}, inplace = True)\n",
    "\n",
    "#Keeping only water level columns which has been named with site names\n",
    "olcdflvl = olcdflvl[['Olcott']]\n",
    "capedflvl = capedflvl[['Cape Vincent']]\n",
    "ozdflvl = ozdflvl[['Oswego']]\n",
    "rochdflvl = rochdflvl[['Rochester']]\n",
    "\n",
    "#resamlping to ensure all days are covered, and linearly interpolating any missing data\n",
    "olcdflvl = olcdflvl.resample('1D').interpolate('linear') #large stretch from mid '99 to mid '00 missing\n",
    "capedflvl = capedflvl.resample('1D').interpolate('linear')\n",
    "ozdflvl = ozdflvl.resample('1D').interpolate('linear')\n",
    "rochdflvl = rochdflvl.resample('1D').interpolate('linear')\n",
    "\n",
    "#Joining into one main dataframe with all waterlevel data\n",
    "waterlevel = pd.concat([olcdflvl, capedflvl, ozdflvl, rochdflvl], axis = 1) \n",
    "\n",
    "#Calculating standard dev\n",
    "stdev = waterlevel.std(axis = 1)\n",
    "\n",
    "#Getting average lake level\n",
    "avgLL = pd.DataFrame()\n",
    "avgLL['avg'] = waterlevel.mean(axis = 1)\n",
    "\n",
    "maximumwldiff = max(stdev) #ft\n",
    "avgwldiff = stdev.mean()\n",
    "maximumwldiffdate = datetime.datetime(2000, 2, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942b295",
   "metadata": {},
   "source": [
    "Discharge data was collected from the United States Geological Survey's Water Data database. The data was gathered as average instantaneous values in cubic feet per second (cfs) and the extrapolated to cubic feet per day, which was converted to total discharge depth based on the gaging site's drainage area. A For-Loop was used to prevent continued division of the resultant discharge depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing Discharge data\n",
    "\n",
    "niagdscg = pd.read_csv(dscgfiles[0], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "niagdscg = niagdscg[['14n']]\n",
    "niagdscg.rename(columns={'14n':'Q Niag'}, inplace = True)\n",
    "\n",
    "\n",
    "genndscg = pd.read_csv(dscgfiles[2], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "genndscg = genndscg[['14n']]\n",
    "genndscg.rename(columns={'14n':'Q Genn'}, inplace = True)\n",
    "\n",
    "\n",
    "blackdscg = pd.read_csv(dscgfiles[3], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "blackdscg = blackdscg[['14n']]\n",
    "blackdscg.rename(columns={'14n':'Q Black'}, inplace = True)\n",
    "\n",
    "\n",
    "ozdscg = pd.read_csv(dscgfiles[4], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozdscg = ozdscg[['14n']]\n",
    "ozdscg.rename(columns={'14n':'Q Oz'}, inplace = True)\n",
    "\n",
    "\n",
    "lawrencedscg = pd.read_csv(dscgfiles[1], delimiter='\\t', comment='#', header=1, \n",
    "                 parse_dates=['20d'], index_col= '20d', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "lawrencedscg = lawrencedscg[['14n']]\n",
    "lawrencedscg['14n'] = lawrencedscg['14n']*86400\n",
    "lawrencedscg.rename(columns={'14n':'Q Out'}, inplace = True)\n",
    "\n",
    "\n",
    "discharge = pd.concat([niagdscg, lawrencedscg, genndscg, blackdscg, ozdscg], axis = 1)\n",
    "dischargedepth = discharge.copy()\n",
    "\n",
    "if dischargedepth.iloc[0,0] > 1:\n",
    "    dischargedepth['Q Niag'] = dischargedepth['Q Niag'] / niagft # average discharge depth per second\n",
    "    dischargedepth['Q Genn'] = dischargedepth['Q Genn'] / gennft\n",
    "    dischargedepth['Q Black'] = dischargedepth['Q Black'] / blkft\n",
    "    dischargedepth['Q Oz'] = dischargedepth['Q Oz'] / ozft\n",
    "    dischargedepth['Q Out'] = dischargedepth['Q Out'] / stlft\n",
    "else:\n",
    "    pass\n",
    "\n",
    "discharge['qsum'] = discharge.iloc[:, [0, 2, 3, 4]].sum(axis = 1)\n",
    "discharge['Q In'] = discharge['qsum']*86400 #cubic feet per day\n",
    "qdiff = discharge['Q In'] - discharge['Q Out']\n",
    "\n",
    "dischargedepth['qsum'] = dischargedepth.iloc[:, [0, 2, 3, 4]].sum(axis = 1)\n",
    "dischargedepth['Q In'] = dischargedepth['qsum']*86400 #cubic feet per day\n",
    "qdiffdepth = dischargedepth['Q In'] - dischargedepth['Q Out']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45644ef3",
   "metadata": {},
   "source": [
    "Precipitation data for three sites were collected from NOAA's Climate Data Online database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c86c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing precip data\n",
    "\n",
    "ozprcp = pd.read_csv(prcpfile[0], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "ozprcp = ozprcp[['PRCP']]\n",
    "ozprcp.rename(columns={'PRCP':'Oz P'}, inplace = True)\n",
    "ozprcp = ozprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "rochprcp = pd.read_csv(prcpfile[1], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "rochprcp = rochprcp[['PRCP']]\n",
    "rochprcp.rename(columns={'PRCP':'Roch P'}, inplace = True)\n",
    "rochprcp = rochprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "wtprcp = pd.read_csv(prcpfile[2], delimiter=',', comment='#', header=0, \n",
    "                 parse_dates=['DATE'], index_col= 'DATE', na_values = \n",
    "                  [2.0181e+11, 2.01902e+11, -9999, 9999, 'NaN', 'Ice', 'Eqp'])\n",
    "wtprcp = wtprcp[['PRCP']]\n",
    "wtprcp.rename(columns={'PRCP':'WT P'}, inplace = True)\n",
    "wtprcp = wtprcp.resample('1D').asfreq().fillna(0) #filling NaNs with 0\n",
    "\n",
    "precipitation = pd.concat([ozprcp, rochprcp, wtprcp], axis = 1)\n",
    "\n",
    "prcptotal = precipitation.sum(axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc40fb",
   "metadata": {},
   "source": [
    "Two data frames of all data were made through concatenation, one using average discharge rates per day and the other using discharge converted to depth to better show the relationship between water depth addition and drawdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1370931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Making one complete dataframe with average water level, combined discharge\n",
    "#and total precipitation\n",
    "\n",
    "dfall = pd.concat([avgLL, prcptotal, qdiff], axis = 1)\n",
    "dfall.rename(columns={0:'total'}, inplace = True)\n",
    "dfall.rename(columns={1:'q'}, inplace = True)\n",
    "dfall['Q Out'] = discharge['Q Out']\n",
    "dfall['Q In'] = discharge['Q In']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5520ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Making one complete data frame but with discharge converted to total \n",
    "#depth and precipitation converted to feet for ease of comparison\n",
    "\n",
    "dfalldepth = pd.concat([avgLL, prcptotal, qdiff], axis = 1)\n",
    "dfalldepth.rename(columns={0:'total'}, inplace = True)\n",
    "dfalldepth.rename(columns={1:'q'}, inplace = True)\n",
    "dfalldepth['total'] = dfalldepth['total'] / 12 # converting rainfall depth from inch to ft\n",
    "dfalldepth['Q Out'] = dischargedepth['Q Out']\n",
    "dfalldepth['Q In'] = dischargedepth['Q In'] + dfall['total']\n",
    "dfalldepth['dif'] = dfalldepth['Q In'] - dfalldepth['Q Out']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf42ee5",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Defining flood events and years\n",
    "\n",
    "#plan 2014 was agreed to and enacted in 2016, previous plan was 1958D which was\n",
    "# in use since 1963 (floods 1973-1998 all plan 1958D)\n",
    "floodyear = pd.DataFrame()\n",
    "floodyear['start'] = ['1973-01-01', '1976-01-01','1983-01-01', \n",
    "                      '1998-01-01','2017-01-01', '2019-01-01',]\n",
    "floodyear['end'] = ['1973-12-31', '1976-12-31', '1983-12-31', \n",
    "                    '1998-12-31','2017-12-31', '2019-12-31']\n",
    "\n",
    "floodyear['start'] = pd.to_datetime(floodyear['start'])\n",
    "floodyear['end'] = pd.to_datetime(floodyear['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "title = 'Data Comparison for Lake Ontario for '\n",
    "\n",
    "def timeplot(df, startdates, enddates):\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex = True, figsize = (11,8))\n",
    "    \n",
    "    #plotting change in discharge (negative = more out, positive = more in)\n",
    "    ax1.plot(df['avg'], label = 'Lake Height')\n",
    "    ax1.set_ylabel('Lake Height (ft)')\n",
    "    ax1.twinx().plot(df['q'], color = 'r', label = 'Delta Discharge')\n",
    "    #ax1.set_ylabel('Change in Discharge (cf/day)')\n",
    "    \n",
    "    #plotting precip \n",
    "    ax2.plot(df['avg'])\n",
    "    ax2.twinx().plot(df['total'], color = 'k', label = 'Precip')\n",
    "    ax2.set_ylabel('Lake Height (ft)')\n",
    "    #ax2.set_ylabel('Precipitation (in)')\n",
    "    \n",
    "    #plotting all NEED TO FIGURE OUT THIRD AXIS (CODE BELOW) \n",
    "    ax3.twinx().plot(df['total'], color = 'k')\n",
    "    ax3.twinx().plot(df['dif'], color = 'r')\n",
    "    ax3.plot(df['avg'])\n",
    "    ax3.set_ylabel('Lake Height (ft)')\n",
    "    #ax3.set_ylabel('Change in Discharge (cf/day)')\n",
    "    \n",
    "    #Lake level height vs actual discharge out of lake\n",
    "    ax4.plot(df['avg'])\n",
    "    ax4.twinx().plot(df['Q Out'], color = 'r', linestyle = 'dotted', label = \n",
    "                     'Stl Discharge')\n",
    "    ax4.set_ylabel('Lake Height (ft)')\n",
    "    \n",
    "    fig.legend(bbox_to_anchor=(1.085,0.5))\n",
    "    fig.suptitle(title + str(startdates.date()) + ' - ' + str(enddates.date()), size = 24)\n",
    "    \n",
    "    \n",
    "timeplot(dfall, startdate, enddate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ed683",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(floodyear['start']):\n",
    "    finaldate = floodyear.iloc[i,1]\n",
    "    loop = dfall[v:finaldate]\n",
    "    timeplot(loop, v, finaldate)\n",
    "    peaklldate = loop['avg'].idxmax()\n",
    "    peakdscgdate = loop['Q Out'].idxmax()\n",
    "    print(peaklldate)\n",
    "    print(peakdscgdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Getting peak lake level date and peak stl discharge date\n",
    "    \n",
    "'''\n",
    "max lake levels during storm years\n",
    "1973-06-01 00:00:00\n",
    "1976-05-27 00:00:00\n",
    "1983-05-26 00:00:00\n",
    "1998-04-17 00:00:00\n",
    "2017-05-26 00:00:00\n",
    "2019-06-14 00:00:00 <--- this was the highest lake level in record\n",
    "    \n",
    "max discharge from STL dates during storm years\n",
    "1973-06-01 00:00:00\n",
    "1973-07-11 00:00:00\n",
    "1976-05-27 00:00:00\n",
    "1976-06-22 00:00:00\n",
    "1983-05-26 00:00:00\n",
    "1983-05-28 00:00:00\n",
    "1998-04-17 00:00:00\n",
    "1998-03-17 00:00:00\n",
    "2017-05-26 00:00:00\n",
    "2017-06-18 00:00:00\n",
    "2019-06-14 00:00:00\n",
    "2019-06-14 00:00:00\n",
    "    \n",
    "    \n",
    "peak discharge value \t1993-05-20\t32,659,200,000 (cubic feet per day)\n",
    "\n",
    "    \n",
    "    \n",
    "maybe iloc into 30 days before and 30 days after peak LL and determine average \n",
    "discharge, then take that as a proportion of average yearly discharge to act \n",
    "as a reference for increase in discharge in response to flood events\n",
    "maybe get the time to return to normal discharge or lake levels\n",
    "    \n",
    "look at discharge rates from each river during storm years to quantify diff from\n",
    "the average whole record discharge\n",
    "    \n",
    "\n",
    "maybe instead of flat 30 days do 30, 15, 5 to see differences between the ramp up/ slow down\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Working out the function (not completely finished yet probably wont run)\n",
    "\n",
    "peakLL = dfall['avg'].idxmax()\n",
    "\n",
    "MonthBeforePeakLL = peakLL - pd.Timedelta(days = 30)\n",
    "BeforePeak15 = peakLL - pd.Timedelta(days = 15)\n",
    "BeforePeak5 = peakLL - pd.Timedelta(days = 5)\n",
    "Temp30PreDscg = dfall.loc[MonthBeforePeakLL : peakLL, 'Q Out']\n",
    "Pre30Avg = Temp30PreDscg.mean()\n",
    "\n",
    "AfterPeak5 = peakLL + pd.Timedelta(days = 5)\n",
    "AfterPeak15 = peakLL + pd.Timedelta(days = 15)\n",
    "MonthAfterPeakLL = peakLL + pd.Timedelta(days = 30)\n",
    "Temp30PostDscg = dfall.loc[peakLL : MonthAfterPeakLL, 'Q Out']\n",
    "Post30Avg = Temp30PostDscg.mean()\n",
    "\n",
    "\n",
    "\n",
    "PercentDifference = abs((PreAvg - PostAvg)/((PostAvg + PreAvg)/2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ef5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creating the function to analyze storms selected\n",
    "def analyzestormyear(totalq, start, end):\n",
    "    peakLL = dfall['avg'].idxmax()\n",
    "    MonthBeforePeakLL = peakLL - pd.Timedelta(days = 30)\n",
    "    MonthAfterPeakLL = peakLL + pd.Timedelta(days = 30)\n",
    "    TempPreDscg = dfall.loc[MonthBeforePeakLL : peakLL, 'Q Out']\n",
    "    TempPostDscg = dfall.loc[peakLL : MonthAfterPeakLL, 'Q Out']\n",
    "    PreAvg = TempPreDscg.mean()\n",
    "    PostAvg = TempPostDscg.mean()\n",
    "    PercentDifference = abs((PreAvg - PostAvg)/((PostAvg + PreAvg)/2) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b0825",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34bb82",
   "metadata": {},
   "source": [
    "Useful links for writing the report:\n",
    "\n",
    "https://ijc.org/en/loslrb/watershed/plan-comparison-2017-2019 (This one talks about flooding simulations betwen 1958DD and 2014 for year 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd28fc",
   "metadata": {},
   "source": [
    "By analyzing lake height data from the Tides and Currents database of the National Oceanographic and Atmospheric Administration in five locations along the US coastline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba32194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
